Learning Rate: 0.05
Max Iterations: 2000
Training Data%: 90%
Testing Data%: 10%

1. Sigmoid Activation
Training on 90.0% data and testing on 10.0% data using the activation function as sigmoid
Training on 90.0% data and testing on 10.0% data using the activation function as sigmoid
After 2000 iterations, and having learning rate as 0.05, the total error is 100.6611252214229
The final weight vectors are (starting from input to output layers)
[[ 1.24251183 -2.33963016  4.16497223 -2.70552527]
 [ 2.7877963  -5.22056406  2.20073878 -6.67946111]
 [ 3.42168431  5.37913239  1.87792436  0.0259675 ]
 [-0.11978371 -2.60771703  3.62676511 -4.62371032]
 [ 1.06377935  2.23313807  4.51285286  0.11812731]
 [-8.53463721  3.29834898 -2.83863478 -4.76825502]
 [ 3.02007809 -4.12435408  0.74607576 -2.95467806]
 [-2.42154726 -1.83499782 -2.94454324 -1.21784732]
 [-1.70956983 -0.96655358  2.19819347 -1.49676085]]
[[10.30729433  2.16293025]
 [-8.04322611 -7.09228174]
 [-7.19114417 -6.84311691]
 [15.03804734  4.75962919]]
[[-10.65693757]
 [ -4.58868192]]
Testing error sum using activation function as sigmoid: 14.508402445464954

2. Tanh Activation
Training on 90.0% data and testing on 10.0% data using the activation function as tanh
After 2000 iterations, and having learning rate as 0.05, the total error is 181.04169816061201
The final weight vectors are (starting from input to output layers)
[[ -2.59367814  -1.56037393  -2.94695977 -27.81554412]
 [  0.90792043  -3.14369419   1.3060401   16.54249463]
 [ -1.69011188  -2.79014594   5.56036369 -12.13536288]
 [ -3.2455437  -11.95489301  13.68669927 -20.73687614]
 [ -2.42034762 -11.99119447   7.83003054 -15.43605263]
 [ -2.47056809 -10.43026177  12.27683736  -2.41334082]
 [ -3.37975291  13.54177771  -1.54161216  16.07484883]
 [ -2.95143056   0.9574911    2.56720463   6.6581679 ]
 [ -2.64657421 -12.03568447  10.61785576 -21.05022721]]
[[-5.46904146 -2.94331183]
 [-7.68912511  3.26887202]
 [12.08725932 -4.45474178]
 [-4.33825652  3.49456546]]
[[-2.6641698 ]
 [ 0.39743509]]
Testing error sum using activation function as tanh: 46.52313159097612

3. ReLu Activation
Training on 90.0% data and testing on 10.0% data using the activation function as relu
After 2000 iterations, and having learning rate as 0.05, the total error is 126.18814749780508
The final weight vectors are (starting from input to output layers)
[[ 0.61667749 -0.26324629  0.21841398 -0.93030455]
 [-0.29084554 -0.84296007  0.3863704  -0.97457467]
 [-0.08090943  0.92263452 -0.33162956 -0.05583316]
 [-0.7892175   0.00615181  0.77137972  0.06875468]
 [-0.43704647 -0.29083062  0.7925605  -0.51702172]
 [-0.95223181  0.93145361 -0.1400642  -0.30742297]
 [ 0.15413526 -0.74694768  0.90008662 -0.37277834]
 [ 0.90566225 -0.56321354 -0.5025996   0.72760359]
 [-0.52946334  0.63020011  0.08779447 -0.61747946]]
[[ 0.17767988 -0.90372641]
 [-0.96825404 -0.90057797]
 [-0.20116942  0.15380792]
 [ 0.73503491  0.57232448]]
[[-0.49059449]
 [-0.83549019]]
Testing error sum using activation function as relu: 16.811852502194906