Learning Rate: 1.0
Max Iterations: 2000
Training Data%: 90%
Testing Data%: 10%

1. Sigmoid Activation
Training on 90.0% data and testing on 10.0% data using the activation function as sigmoid
After 2000 iterations, and having learning rate as 1, the total error is 126.1881465311347
The final weight vectors are (starting from input to output layers)
[[ 0.6623599  -0.47358808  0.21778691 -0.70305828]
 [-0.30493746 -0.64342036  0.3917997  -1.07454414]
 [ 0.11450023  1.2810013  -0.34932164 -0.40566644]
 [-0.55355147  0.40753194  0.75112117 -0.40559247]
 [-0.13959614  0.1894893   0.77554981 -1.07253344]
 [-0.80866038  1.40058953 -0.16246356 -0.8711257 ]
 [ 0.20431916 -0.82153163  0.9117865  -0.15224502]
 [ 1.05513596 -0.25370798 -0.50059695  0.45939726]
 [-0.37474799  0.91416744  0.0712692  -1.08163431]]
[[ 2.25674191  2.41332713]
 [-0.03714029  0.40914351]
 [ 0.83735713  1.81080503]
 [ 2.45159075  3.30455568]]
[[-13.13098412]
 [ -9.22233603]]
Testing error sum using activation function as sigmoid: 16.811852400157985

2. Tanh Activation
Training on 90.0% data and testing on 10.0% data using the activation function as tanh
After 2000 iterations, and having learning rate as 1, the total error is 214.3009079398663
The final weight vectors are (starting from input to output layers)
[[   -39177.91446434    662861.54425137   1288449.13041744
   -4820642.9112369 ]
 [ -3889398.42976356  -4403108.80794495     78994.89374014
    7803108.61618336]
 [  -732168.83085916    540770.34615579  -2503810.12426889
    2019390.9183479 ]
 [  -832543.66079068    479975.34401963  -9027659.66946127
    5711927.89036879]
 [   539769.49404063   -793452.20332979 -10365352.5371785
    4246269.96165747]
 [  1497936.57422372  -1155474.50735602  -9984238.68083335
    2345853.20129422]
 [ -6699886.90863707   -563740.52455295   6939413.18547492
    4499540.2713032 ]
 [ -1501014.49205876   -794479.43452777    430977.940952
    2145558.13215789]
 [ -1075821.2573806    -566010.46333467  -6059761.79526316
    4082424.4436914 ]]
[[ -995.59141042 -4855.66060193]
 [ 2166.6609648  -3026.11181425]
 [ 4013.24879613  2116.59961004]
 [ -103.93774689  6550.4305472 ]]
[[10.06892876]
 [20.10428038]]
Testing error sum using activation function as tanh: 24.571720225585032

3. ReLu Activation
Training on 90.0% data and testing on 10.0% data using the activation function as relu
After 2000 iterations, and having learning rate as 1, the total error is 126.18814749780508
The final weight vectors are (starting from input to output layers)
[[ 0.61667749 -0.26324629  0.21841398 -0.93030455]
 [-0.29084554 -0.84296007  0.3863704  -0.97457467]
 [-0.08090943  0.92263452 -0.33162956 -0.05583316]
 [-0.7892175   0.00615181  0.77137972  0.06875468]
 [-0.43704647 -0.29083062  0.7925605  -0.51702172]
 [-0.95223181  0.93145361 -0.1400642  -0.30742297]
 [ 0.15413526 -0.74694768  0.90008662 -0.37277834]
 [ 0.90566225 -0.56321354 -0.5025996   0.72760359]
 [-0.52946334  0.63020011  0.08779447 -0.61747946]]
[[ 0.17767988 -0.90372641]
 [-0.96825404 -0.90057797]
 [-0.20116942  0.15380792]
 [ 0.73503491  0.57232448]]
[[-0.49059449]
 [-0.83549019]]
Testing error sum using activation function as relu: 16.811852502194906