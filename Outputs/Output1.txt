Learning Rate: 0.05
Max Iterations: 1000
Training Data%: 75%
Testing Data%: 25%

1. Sigmoid Activation
Training on 75.0% data and testing on 25.0% data using the activation function as sigmoid
After 1000 iterations, and having learning rate as 0.05, the total error is 88.50282234130717
The final weight vectors are (starting from input to output layers)
[[ 0.63199063 -2.05009171 -0.35852763 -1.81793948]
 [-3.52849965  1.36484026 -0.16946856 -2.7951918 ]
 [ 1.75845522  5.59920521  1.79062981 -1.42173017]
 [-3.40738224  1.56468741  2.19016997 -1.80119335]
 [-0.79016001 -1.8487851   0.94093419 -4.07663383]
 [-2.09903787  4.19164151  0.93786764 -2.79617184]
 [ 0.46557341 -3.55625284  1.64419637  0.15836082]
 [-0.8295524   3.77643484 -1.30602841 -1.00026005]
 [-2.60826904  2.1943264   1.79553925 -3.13274971]]
[[ 5.54331954  2.14772646]
 [-7.43384011 -7.90117509]
 [-2.06704327  0.19160629]
 [ 8.27896238  4.53877277]]
[[-8.12752157]
 [-5.49834951]]
Testing error sum using activation function as sigmoid: 34.22431562572993

2. Tanh Activation
Training on 75.0% data and testing on 25.0% data using the activation function as tanh
After 1000 iterations, and having learning rate as 0.05, the total error is 178.90041754398618
The final weight vectors are (starting from input to output layers)
[[ -4.37681282  -3.67313868   1.87817515  23.20210429]
 [ -4.34323148   8.29366503   2.93924942 -10.86526026]
 [ -5.84718676  14.62241539   0.65248986 -16.52190448]
 [  2.87551071  12.71650685   0.49248398 -38.86043072]
 [  7.75337398  20.35570748   0.8169175  -37.1532589 ]
 [ -3.43894395  20.35694812   1.9209597  -32.91909323]
 [-10.76610637  -3.65278405   2.35035642   9.26034856]
 [  5.0656458   15.33849583  -0.19703206  -0.60310802]
 [ -5.07037167  14.92349026   0.53059312 -28.3761161 ]]
[[  4.88842269   1.15364926]
 [  0.02935361   1.72990171]
 [ -3.49301502 -14.98897174]
 [-12.02853824 -14.29802453]]
[[-2.56116694]
 [ 9.3211023 ]]
Testing error sum using activation function as tanh: 71.88932346790031

3. ReLu Activation
Training on 75.0% data and testing on 25.0% data using the activation function as relu
After 1000 iterations, and having learning rate as 0.05, the total error is 107.38700614574185
The final weight vectors are (starting from input to output layers)
[[ 0.61667749 -0.26324629  0.21841398 -0.93030455]
 [-0.29084554 -0.84296007  0.3863704  -0.97457467]
 [-0.08090943  0.92263452 -0.33162956 -0.05583316]
 [-0.7892175   0.00615181  0.77137972  0.06875468]
 [-0.43704647 -0.29083062  0.7925605  -0.51702172]
 [-0.95223181  0.93145361 -0.1400642  -0.30742297]
 [ 0.15413526 -0.74694768  0.90008662 -0.37277834]
 [ 0.90566225 -0.56321354 -0.5025996   0.72760359]
 [-0.52946334  0.63020011  0.08779447 -0.61747946]]
[[ 0.17767988 -0.90372641]
 [-0.96825404 -0.90057797]
 [-0.20116942  0.15380792]
 [ 0.73503491  0.57232448]]
[[-0.49059449]
 [-0.83549019]]
Testing error sum using activation function as relu: 35.61299385425811