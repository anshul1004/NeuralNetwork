Learning Rate: 1.0
Max Iterations: 1000
Training Data%: 80%
Testing Data%: 20%

1. Sigmoid Activation
Training on 80.0% data and testing on 20.0% data using the activation function as sigmoid
After 1000 iterations, and having learning rate as 1, the total error is 113.25986633436756
The final weight vectors are (starting from input to output layers)
[[ 0.62884212 -0.55688097  0.24088487 -0.60267434]
 [-0.29254494 -0.6284343   0.37812742 -1.12504324]
 [ 0.09448303  1.24531675 -0.3545513  -0.41634247]
 [-0.55565349  0.49890842  0.73710788 -0.42787097]
 [-0.14774399  0.27429038  0.74718863 -1.14153377]
 [-0.85876991  1.33147663 -0.18573812 -0.8272396 ]
 [ 0.21701035 -0.80311333  0.92843999 -0.19104297]
 [ 1.09012979 -0.22754091 -0.49785285  0.43512517]
 [-0.41790639  0.90306144  0.05002441 -1.05910423]]
[[ 2.04209451  2.06003214]
 [-0.2604163   0.18728094]
 [ 0.55215115  1.42141426]
 [ 2.20735502  2.90190126]]
[[-11.33562586]
 [ -7.80142092]]
Testing error sum using activation function as sigmoid: 29.740055128498934

2. Tanh Activation
Training on 80.0% data and testing on 20.0% data using the activation function as tanh
After 1000 iterations, and having learning rate as 1, the total error is 303.1227633213832
The final weight vectors are (starting from input to output layers)
[[-1467433.35698052   -32285.62692498  1083054.48960362  -325983.69569468]
 [ 1014730.88686881  -198057.55691298 -1020867.68664717   518898.17073721]
 [ 3430209.19070755 -1434118.51037207 -1497708.92162371  1935792.70323962]
 [ 5918506.19710901 -2050300.99840406 -3075837.97218624  3079592.79322175]
 [ 5282471.59774585 -1204392.27917374 -2945491.04285057  2838946.80274896]
 [ 5532355.70099777 -1859771.53224141 -2890332.16399429  2761154.43745264]
 [-1187494.1700058   -410671.71904719   531963.09725829  -897930.73715763]
 [ 1021712.27127582  -717598.23197215   -50382.09070039   469986.27495088]
 [ 4358630.10577434 -1731211.53843718 -1964465.76064725  2072519.99652092]]
[[ 6068.4557501   3218.18285227]
 [  534.41437706  5341.82317341]
 [  -83.7553846   4929.91734023]
 [  -34.75620984 -5317.04391062]]
[[-32.90262995]
 [170.56622907]]
Testing error sum using activation function as tanh: 70.09347667871174

3. ReLu Activation
Training on 80.0% data and testing on 20.0% data using the activation function as relu
After 1000 iterations, and having learning rate as 1, the total error is 113.25993561603744
The final weight vectors are (starting from input to output layers)
[[ 0.61667749 -0.26324629  0.21841398 -0.93030455]
 [-0.29084554 -0.84296007  0.3863704  -0.97457467]
 [-0.08090943  0.92263452 -0.33162956 -0.05583316]
 [-0.7892175   0.00615181  0.77137972  0.06875468]
 [-0.43704647 -0.29083062  0.7925605  -0.51702172]
 [-0.95223181  0.93145361 -0.1400642  -0.30742297]
 [ 0.15413526 -0.74694768  0.90008662 -0.37277834]
 [ 0.90566225 -0.56321354 -0.5025996   0.72760359]
 [-0.52946334  0.63020011  0.08779447 -0.61747946]]
[[ 0.17767988 -0.90372641]
 [-0.96825404 -0.90057797]
 [-0.20116942  0.15380792]
 [ 0.73503491  0.57232448]]
[[-0.49059449]
 [-0.83549019]]
Testing error sum using activation function as relu: 29.740064383962526