Learning Rate: 0.5
Max Iterations: 2000
Training Data%: 80%
Testing Data%: 20%

1. Sigmoid Activation
Training on 80.0% data and testing on 20.0% data using the activation function as sigmoid
After 2000 iterations, and having learning rate as 0.5, the total error is 93.1720275989999
The final weight vectors are (starting from input to output layers)
[[ 0.70018937  3.76713202  2.10508645 -2.61357037]
 [-5.07791836  3.49999769 -0.11581053 -4.15603998]
 [ 3.896289    8.32834914  3.42842989 -1.33250942]
 [-4.26954664  5.66110039  4.82852489 -0.65780841]
 [-0.03876726  3.24097616  3.59138378 -6.64258437]
 [-4.42986789  0.92721733 -3.31709343 -4.74179432]
 [-0.52669048 -5.47718387  2.678217    0.14606725]
 [-1.11253309  2.21931438 -3.68145707 -1.06651801]
 [-1.9081865   2.81033634  1.96678208 -5.31027692]]
[[ 11.06498022   2.29298576]
 [-13.58426566 -11.60148907]
 [ -7.00769574  -5.66978352]
 [ 11.59713825   2.95612815]]
[[-13.70940661]
 [ -5.79879268]]
Testing error sum using activation function as sigmoid: 29.668119425618666

2. Tanh Activation
Training on 80.0% data and testing on 20.0% data using the activation function as tanh
After 2000 iterations, and having learning rate as 0.5, the total error is 239.97514451159736
The final weight vectors are (starting from input to output layers)
[[   6201.96233609   92594.16202527    3350.55380214   -6096.1155941 ]
 [   8929.5890742   -63742.66123452   -9868.6366312    -6487.71250345]
 [ -50811.46103669  -87216.25846665   19454.93438086   96817.43814551]
 [-199057.07101128 -191806.89502908   20729.56268673   63514.32784928]
 [-207833.44421053 -196374.00268413   22412.85175192   56989.43349074]
 [-181123.09772626 -154207.06131163   30565.78887572   61181.95989139]
 [  51022.60275665   56344.11172548  -10731.0893799     8006.35586266]
 [ -11250.30610131  -22056.06065741    1095.03984238   53664.67507534]
 [-151247.04118388 -112940.81627547   22990.91827233   71417.59848277]]
[[-441.71790321 -202.21300965]
 [ 131.50622558 -244.74694661]
 [ 283.37196723  680.58468555]
 [ 260.75483864 -398.04185248]]
[[ 25.42384943]
 [-39.02931683]]
Testing error sum using activation function as tanh: 70.56781061535165

3. ReLu Activation
Training on 80.0% data and testing on 20.0% data using the activation function as relu
After 2000 iterations, and having learning rate as 0.5, the total error is 113.25993561603744
The final weight vectors are (starting from input to output layers)
[[ 0.61667749 -0.26324629  0.21841398 -0.93030455]
 [-0.29084554 -0.84296007  0.3863704  -0.97457467]
 [-0.08090943  0.92263452 -0.33162956 -0.05583316]
 [-0.7892175   0.00615181  0.77137972  0.06875468]
 [-0.43704647 -0.29083062  0.7925605  -0.51702172]
 [-0.95223181  0.93145361 -0.1400642  -0.30742297]
 [ 0.15413526 -0.74694768  0.90008662 -0.37277834]
 [ 0.90566225 -0.56321354 -0.5025996   0.72760359]
 [-0.52946334  0.63020011  0.08779447 -0.61747946]]
[[ 0.17767988 -0.90372641]
 [-0.96825404 -0.90057797]
 [-0.20116942  0.15380792]
 [ 0.73503491  0.57232448]]
[[-0.49059449]
 [-0.83549019]]
Testing error sum using activation function as relu: 29.740064383962526