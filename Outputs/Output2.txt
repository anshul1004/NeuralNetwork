Learning Rate: 0.5
Max Iterations: 1000
Training Data%: 75%
Testing Data%: 25%

1. Sigmoid Activation
Training on 75.0% data and testing on 25.0% data using the activation function as sigmoid
After 1000 iterations, and having learning rate as 0.5, the total error is 86.35884827755144
The final weight vectors are (starting from input to output layers)
[[-0.69097273 -3.25138249 -1.86191196 -7.67491353]
 [-5.8083822   1.8786648  -0.84900562 -1.91177033]
 [ 0.4988225   7.89820155  6.92565475  4.79147539]
 [-4.58354842  2.46007521  5.81805605 -0.03854182]
 [-3.79162356 -2.58099547  1.50692675 -6.29555472]
 [-2.99805118  6.23490456 -0.85905282 -5.26828998]
 [ 1.08198687 -4.89471184  0.76653168 -0.48927485]
 [-0.51458885  4.81391811 -2.44234355 -4.42723981]
 [-4.00471351  3.77198125  4.86422612 -1.57520221]]
[[ 11.53862365   0.56646885]
 [-10.06221587 -13.78137479]
 [-11.11689159   2.57661759]
 [ 12.03759727   1.38501974]]
[[-12.13938246]
 [ -8.74739202]]
Testing error sum using activation function as sigmoid: 34.15727839185308

2. Tanh Activation
Training on 75.0% data and testing on 25.0% data using the activation function as tanh
After 1000 iterations, and having learning rate as 0.5, the total error is 267.25228846158893
The final weight vectors are (starting from input to output layers)
[[ -80528.98394004  -83732.45646355   -1806.91428538  156048.36976688]
 [  93484.0980167    83775.04938807   60309.70765216  -90405.70610855]
 [  76854.60490354  134788.15344658   48079.41528487 -201210.93639301]
 [ 221262.49628899  216460.71147478  109324.46425794 -272884.80079354]
 [ 120899.8297401   167644.16599614   95546.5277454  -256541.43266964]
 [  94595.75555338  215606.85465346   26812.10926378 -253813.55719217]
 [ 178417.50061838 -118001.63741792  153014.22452488   72470.7710599 ]
 [  54417.13258038  -42130.35341739  148161.44227579  -39837.56393733]
 [ 236963.63596301  128043.54421464  163633.22015218 -254716.02568608]]
[[ 324.35877547  -13.64623776]
 [-947.5117041   663.46849652]
 [ 666.63039327 -862.10106508]
 [ 918.93615446 -344.39678226]]
[[24.98799077]
 [ 3.79107989]]
Testing error sum using activation function as tanh: 87.75564880164411

3. ReLu Activation
Training on 75.0% data and testing on 25.0% data using the activation function as relu
After 1000 iterations, and having learning rate as 0.5, the total error is 107.38700614574185
The final weight vectors are (starting from input to output layers)
[[ 0.61667749 -0.26324629  0.21841398 -0.93030455]
 [-0.29084554 -0.84296007  0.3863704  -0.97457467]
 [-0.08090943  0.92263452 -0.33162956 -0.05583316]
 [-0.7892175   0.00615181  0.77137972  0.06875468]
 [-0.43704647 -0.29083062  0.7925605  -0.51702172]
 [-0.95223181  0.93145361 -0.1400642  -0.30742297]
 [ 0.15413526 -0.74694768  0.90008662 -0.37277834]
 [ 0.90566225 -0.56321354 -0.5025996   0.72760359]
 [-0.52946334  0.63020011  0.08779447 -0.61747946]]
[[ 0.17767988 -0.90372641]
 [-0.96825404 -0.90057797]
 [-0.20116942  0.15380792]
 [ 0.73503491  0.57232448]]
[[-0.49059449]
 [-0.83549019]]
Testing error sum using activation function as relu: 35.61299385425811